pyarpack: python binding based on Boost.Python.Numpy used to expose arpack C++ API

Installation:
-------------

Python3: ~/arpack-ng/build> cmake -DCMAKE_INSTALL_PREFIX=/tmp/local -DPYTHON3=ON -DBOOST_PYTHON_LIBSUFFIX="3" ..
         ~/arpack-ng/build> make all test
         ~/arpack-ng/build> make install
Note: Boost must have been compiled for Python3.
Note: installation will install libpyarpack.so in the install directory.
Note: python3 minimal packages must be installed (apt-get install python3-minimal python3-pip).
      numpy must be installed (apt-get install python3-numpy) as the A/B matrices used by arpack
      solver must be numpy arrays.

Usage:
------

PYTHONPATH must include the directory where libpyarpack.so has been installed.

>> export PYTHONPATH="/tmp/local/lib/pyarpack:${PYTHONPATH}"
>> python
>> import pyarpack
>> help(pyarpack)

You can use sparse or dense matrices, and, play with iterative or direct mode solvers (CG, LU, ...):

1. choose arpack solver with a given mode solver
   1.1. if you need to handle sparse matrices
        >> from pyarpack import sparseBiCG as pyarpackSlv
   1.2. if you need to handle dense matrices
        >> from pyarpack import denseBiCG as pyarpackSlv
2. choose arpack data type (float, double, ...)
   >> arpackSlv = pyarpackSlv.double()
3. solve the eigen problem
   >> arpackSlv.solve(A, B)
4. get eigen values and vectors
   >> print(arpackSlv.vec)
   >> print(arpackSlv.val)

You can also:

1. restart a solve from the workspace of a previous solve: check out pyarpackRestart.py.in.
2. compute eigen and / or schur vectors.

Note:

1. arpack data type (float, double, ...) must be consistent with A/B numpy dtypes (float32, float64, ...).
   at python side, the data MUST be casted in the EXACT expected type (int32, int64, float, double, ...).
   otherwise, C++ may not get the data the way it expects them: C++ will not know how to read python data.
   if you are not sure how data have been passed from python to C++, set arpackSlv.debug = 1 and check out debug traces.
   in other words, pyarpack users MUST :
   1.1. create numpy arrays specifying explicitly the type:
        >> Aij = np.array([], dtype='complex128')
   1.2. filling numpy arrays casting value on append:
        >> Aij = np.append(Aij, np.complex128(np.complex( 200.,  200.))) # Casting value on append is MANDATORY or C++ won't get the expected type.
   1.3. calling the solver flavor which is consistent with the numpy array data type:
        >> arpackSlv = pyarpackSlv.complexDouble() # Caution: complexDouble <=> np.array(..., dtype='complex128')
   note: NO data type check can be done at C++ side, the pyarpack user MUST insure data consistency.
2. sparse matrices must be provided in coo format (n, i, j, Mij), that is, as a tuple where:
   2.1. n is an integer.
   2.2. i, j, Mij are 1 x nnz numpy arrays.
3. dense  matrices must be provided in raw format (Mij, rowOrdered), that is, as a tuple where:
   3.1. Mij is an n x n numpy array.
   3.2. rowOrdered is a boolean (column ordered if False).
4. arpack mode solver are provided by eigen:
   4.1. when solver is iterative, A and B can be sparse only.
   4.2. when solver is direct,    A and B can be sparse or dense.

Examples:
---------

~/arpack-ng> find . -name *.py.in (template files from which python scripts will result)

